# -*- coding: utf-8 -*-
"""Finetuning_model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QHz22uJXADGMuXjRtx-m6opqDbd9fX9
"""

!pip install -U transformers datasets peft trl bitsandbytes accelerate

from datasets import load_dataset

# Load and shuffle, then select 1,000 samples for quick training
dataset = load_dataset("ruslanmv/ai-medical-chatbot", split="train")
dataset = dataset.shuffle(seed=42).select(range(1000))

import os
os.environ["WANDB_DISABLED"] = "true"

from huggingface_hub import login
login()  # Paste your token when prompted

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

def format_chat(row):
    # Format using the tokenizer's chat template
    messages = [
        {"role": "user", "content": row["Patient"]},
        {"role": "assistant", "content": row["Doctor"]}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

dataset = dataset.map(format_chat)

dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset['train']
eval_dataset = dataset['test']

from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

base_model_id = "meta-llama/Llama-3.2-3B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    load_in_4bit=True,
    device_map="auto"
)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

import os
os.environ["WANDB_DISABLED"] = "true"

from transformers import TrainingArguments
from trl import SFTTrainer

training_args = TrainingArguments(
    per_device_train_batch_size=2,
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=True,
    output_dir="./llama3.2-medical-chat"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,
)

trainer.train()

from huggingface_hub import login
login()  # Paste your token when prompted



from transformers import AutoModelForCausalLM

# Option 1: If you used Trainer/SFTTrainer
trainer.save_model("hippocratically_llama3.2")  # Save to a new directory if needed

from transformers import AutoTokenizer
tokenizer.save_pretrained("hippocratically_llama3.2")

# Push model and tokenizer to your Hugging Face account
model = AutoModelForCausalLM.from_pretrained("hippocratically_llama3.2")
model.push_to_hub("thebnbrkr/hippocratically_llama3.2")
tokenizer.push_to_hub("thebnbrkr/hippocratically_llama3.2")

